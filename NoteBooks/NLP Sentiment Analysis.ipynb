{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb72ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Usage:   \r\n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\r\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\r\n",
      "  pip install [options] [-e] <vcs project url> ...\r\n",
      "  pip install [options] [-e] <local project path> ...\r\n",
      "  pip install [options] <archive url/path> ...\r\n",
      "\r\n",
      "no such option: --y\r\n"
     ]
    }
   ],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"\n",
    "!pip install nltk --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d15371c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "warn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cb824ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "warn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bb6fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Helper Functions \n",
    "# This function will print the class and document dataframe\n",
    "def print_sample_and_class(data, class_=1,n_samples=5):\n",
    "\n",
    "    index=data[data['y']==class_]['X'].index[0:n_samples]\n",
    "\n",
    "    for i in index:\n",
    "        print(\"sample {} of class {}\".format(i,class_))\n",
    "        print(data[data['y']==class_]['X'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82c9536e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The gorgeously elaborate continuation of `` T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>You 'd think by now America would have had en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y                                                  X\n",
       "0  1   The Rock is destined to be the 21st Century '...\n",
       "1  1   The gorgeously elaborate continuation of `` T...\n",
       "2  1   Singer\\/composer Bryan Adams contributes a sl...\n",
       "3  0   You 'd think by now America would have had en...\n",
       "4  1               Yet the act is still charming here ."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=  pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/train.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aabb6145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0 of class 1\n",
      " The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "sample 1 of class 1\n",
      " The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\n",
      "sample 2 of class 1\n",
      " Singer\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .\n",
      "sample 4 of class 1\n",
      " Yet the act is still charming here .\n",
      "sample 5 of class 1\n",
      " Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\n"
     ]
    }
   ],
   "source": [
    "print_sample_and_class(df, class_=1,n_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "676565f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document:  The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "\n",
      " type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "my_string=df['X'][0]\n",
    "print(\"document:\",my_string)\n",
    "print(\"\\n type:\",type(my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95d0d36d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Rock '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71a2c9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Rock',\n",
       " 'is',\n",
       " 'destined',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " '21st',\n",
       " 'Century',\n",
       " \"'s\",\n",
       " 'new',\n",
       " '``',\n",
       " 'Conan',\n",
       " \"''\",\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'splash',\n",
       " 'even',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'Arnold',\n",
       " 'Schwarzenegger',\n",
       " ',',\n",
       " 'Jean-Claud',\n",
       " 'Van',\n",
       " 'Damme',\n",
       " 'or',\n",
       " 'Steven',\n",
       " 'Segal',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can split the string into a individual of words, each word is called a token\n",
    "my_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf7e4893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In python, we can apply different string operations like converting each character to lowercase:\n",
    "my_string.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21dd987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        the rock is destined to be the 21st century '...\n",
       "1        the gorgeously elaborate continuation of `` t...\n",
       "2        singer\\/composer bryan adams contributes a sl...\n",
       "3        you 'd think by now america would have had en...\n",
       "4                    yet the act is still charming here .\n",
       "                              ...                        \n",
       "8539                                      a real snooze .\n",
       "8540                                       no surprises .\n",
       "8541     we 've seen the hippie-turned-yuppie plot bef...\n",
       "8542     her fans walked out muttering words like `` h...\n",
       "8543                                  in this case zero .\n",
       "Name: X, Length: 8544, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can apply this operation to every column by using .str attribute followed by the string operations \n",
    "# we would like to perform.\n",
    "df[\"X\"].str.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a76bff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a list of positive and negative words :\n",
    " # Count the number of \"good words\" and \"bad words\" in the text\n",
    "good_words = ['love', 'good','excellent', 'great','charming']\n",
    "\n",
    "\n",
    "bad_words = ['hate', 'bad','brutal', 'damnable', 'deplorable', 'detestable', 'disastrous', 'dreadful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad688810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assign a 1 to the positive sentiment word and a -1  to all the negative sentiment words, then calculate the\n",
    "# total for each document and place it in the column score:\n",
    "df[\"score\"] = 0\n",
    "for bad_word in bad_words:\n",
    "    df[\"score\"]-= df[\"X\"].str.casefold().str.count(bad_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f18f4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "for good_words in good_words:\n",
    "    df[\"score\"]+=df[\"X\"].str.casefold().str.count(good_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6199f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The gorgeously elaborate continuation of `` T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>You 'd think by now America would have had en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y                                                  X  score\n",
       "0  1   The Rock is destined to be the 21st Century '...      1\n",
       "1  1   The gorgeously elaborate continuation of `` T...      0\n",
       "2  1   Singer\\/composer Bryan Adams contributes a sl...      0\n",
       "3  0   You 'd think by now America would have had en...      0\n",
       "4  1               Yet the act is still charming here .      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are giving each word a score, one for a positive sentiment word and a negative one for negative sentiment word.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6a261a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Good fun , good action , good acting , good dialogue , good pace , good cinematography .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can print out the document with the highest score:\n",
    "df.loc[df[\"score\"].argmax(axis=0),'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1f15925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It is that rare combination of bad writing , bad direction and bad acting -- the trifecta of badness .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also print out the document with the lowest score:\n",
    "df.loc[df[\"score\"].argmin(axis=0),'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f1d6b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.003021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score\n",
       "y           \n",
       "-1  0.003021\n",
       " 0  0.057882\n",
       " 1  0.106094"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We calculate the score for positive and negative and neural classes\n",
    "df.groupby('y').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e3680c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "-1    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       " 0    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       " 1    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       "Name: score, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we see that the scores are relatively the same. We can also plot a histogram we see for each class most of the \n",
    "# samples overlap.\n",
    "df[[\"score\",\"y\"]].groupby('y')[\"score\"].hist(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85e38de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new column yhat for the sentiment we will assign each sample to a neutral sentiment:\n",
    "df['yhat']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa9e65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can set rule-based thresholds if the score is less than the threshold; we set the sentiment to positive\n",
    "# sentiment; similarly, if it's less than the negative of the threshold, we set the sentiment to negative. \n",
    "# Anything between we set to neutral. In this case, the threshold is one; you can try different values for\n",
    "# thresholds or different rules on your own ;\n",
    "\n",
    "negative_label = df[['score']].sum(axis=1) < 1\n",
    "pos_label = df[['score']].sum(axis=1) > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5dc6c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assign the class according the the score:\n",
    "df['yhat'][negative_label] = -1\n",
    "df['yhat'][pos_label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e93e492a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    3610\n",
       "-1    3310\n",
       " 0    1624\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da53cda9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43562734082397003"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We compare the predicted sentiment with the actual sentiment, we see the accuracy is not good\n",
    "np.mean(df['yhat']==df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c815d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many reasons why the rule based classifier did not work. The list of words was small the datasets can \n",
    "# have thousands of words both positive and negative words; also we dint have any neutral words. Are scoring methods\n",
    "# we use gives a one for a positive sentiment word and a negative one for negative sentiment word, maybe some words\n",
    "# should be grater then one for positive sentiment like \"amazing\" should be 10 and ok should be 4, similarly some \n",
    "# words with negative sentiment should have large negative values like \"barfed \" should be \"-10\" and \"boring\" should\n",
    "# be -4. Maybe neutral and negative words may have a score for positive sentiment and vise versa. One way to answer\n",
    "# these questions is to use machine learning to determine these these scores.\n",
    "\n",
    "# Machine Learning\n",
    "# Machine Learning (ML) is a standard tool for NLP tasks. There are many machine learning methods, and in addition,\n",
    "# there are many machine learning methods specially built for NLP tasks. We will use multi-class logistic regression\n",
    "# via scikit-learn , a more general approach. We will for two reasons ; first, if you have any experience in\n",
    "# Machine learning, you probably have experience with logistic regression, second logistic regression can be \n",
    "# interpreted as a kind of scoring. The parameters in this case the scores are obtained via training or learning,\n",
    "# we try different scores called weights for each word until we minimize the miss classified samples. This is done\n",
    "# in an optimal way. It turns out that ML can classify NLP data so well we have to test our method using data we\n",
    "# haven seen, we actually have to test it twice on data it has not see, the datasets form\n",
    "\n",
    "# Training data set A training data set is a data set of examples used during the learning process and is used to \n",
    "# fit the parameters (e.g., weights or scores )\n",
    "\n",
    "# Validation data set A validation data set is a data-set of examples used to tune the hyperparameters these are\n",
    "# related to the learning training and are chose by experimenting\n",
    "\n",
    "# Test data set A test data set is a data set that is independent of the training data set and Validation , it \n",
    "# basicly how good you model should do in the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c22267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/train.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X']) \n",
    "validation_dataset=pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/dev.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X']) \n",
    "test_dataset =pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/test.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ccb11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "# Machines, different from humans, cannot comprehend raw text. Therefore, we have got to transform our content into \n",
    "# numbers we denote features for  ùëõ‚àíùë°‚Ñé document with the feature vector  ùê±ùëõ\n",
    "#  . The  ùëñ‚àíùë°‚Ñé\n",
    "#   word would be the i-th element in  ùê±ùëõ\n",
    "#   denoted by  ùë•ùëñ\n",
    "#   we drop the  ùëõ\n",
    "#   for simplicity. The dataset would be the matrix  ùêó\n",
    "#   where each row corresponds to a document and each column corresponds to a word .Lets see how to transform \n",
    "# documents into corresponding numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf12f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-Of-Words\n",
    "# A bag-Of-Words (BoW) model transforms text into fixed-length vectors; for example, \n",
    "# a count if the number of times the word is present in a document or token counts. \n",
    "# Scikit-learn's ```CountVectorizer```  is one method to perform BoW transformation; it converts a set of\n",
    "# text documents to a matrix of token counts called a document-term matrix (TDM). \n",
    "# The TDM is a sparse matrix object   ```scipy.sparse.csr_matrix```; where each row represents a different document,\n",
    "# and each column represents each word in the document. For each  element in the matrix, represent how many times\n",
    "# that word occurs. First we import  ```CountVectorizer``` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c472d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a58160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " corpus = [\n",
    "    'This is the first of document .',\n",
    "    'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd251eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we apply the CountVectorizer object,the output is the TDM a sparse matrix object:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_toy = vectorizer.fit_transform(corpus)\n",
    "X_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ca5ec36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'of',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can output the word corresponding to each column\n",
    "vectorizer. get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a50610ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can cast the output to a numpy array \n",
    "X_toy.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd112de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  document  first  is  of  one  \\\n",
       "This is the first of document .          0         1      1   1   1    0   \n",
       "This document is the second document.    0         2      0   1   0    0   \n",
       "And this is the third one.               1         0      0   1   0    1   \n",
       "Is this the first document?              0         1      1   1   0    0   \n",
       "\n",
       "                                       second  the  third  this  \n",
       "This is the first of document .             0    1      0     1  \n",
       "This document is the second document.       1    1      0     1  \n",
       "And this is the third one.                  0    1      1     1  \n",
       "Is this the first document?                 0    1      0     1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can convert the output to a datafame. We see the word corresponding to each column and the phrase for each row.\n",
    "original_toy_df= pd.DataFrame(X_toy.toarray(),columns=vectorizer.get_feature_names(),index=corpus )\n",
    "original_toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55fe01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first document 'This is the first of document .' would have the featuer  ùê±1=[0,1,1,1,1,0,0,1,0,1]\n",
    "#  , the second document would be  ùê±2=[0,2,0,1,0,0,1,1,0,1]\n",
    "#   and do on\n",
    "\n",
    "# Training a Model with Grid Search and Logistic Regression¬∂\n",
    "# In this section, let's train a model with Grid Serch and Logistic Regression first we import the class \n",
    "# constructors, you will do a much more simple example in Question 2 where you use default settings of\n",
    "# Logistic Regression.\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# The data is pre-split into training and validation data. As we want to determine hyperparameters automatically\n",
    "# using  ```GridSearchCV``` we need to combine the datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38982b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.concat([train_dataset,validation_dataset],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9f369eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can keep track of what samples belong to training a validation by creating a NumPy array where negative \n",
    "# ones correspond to training samples, and zeros correspond to validation data. We then use a predefined scheme \n",
    "# using PredefinedSplit. \n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "split_index = [-1]*train_dataset.shape[0] + [0]*validation_dataset.shape[0]\n",
    "pds = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09302d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a CountVectorizer() object and transform the dataset:\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(dataset['X'])\n",
    "y = dataset['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bd65197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do the same for the test dataset\n",
    "X_test = vectorizer.transform(test_dataset['X'])\n",
    "y_test = test_dataset[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "745cc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dictionary of hyperparameters, the Inverse of regularization strength; and the norm of the penalty. \n",
    "# We assume L1 would be better as the data is sparse:\n",
    "param_grid = {'penalty' : ['l1', 'l2'],'C' : np.logspace(-4, 4, 20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed481b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid={'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "                         'penalty': ['l1', 'l2']})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  We create a GridSearchCV object , with a logistic regression estimator, the parameter cv \n",
    "# Determines the cross-validation splitting strategy, we use the predefined split. Finely we use the the \n",
    "# parameter grid defined above:\n",
    "clf = GridSearchCV(estimator = LogisticRegression(),cv=pds,param_grid=param_grid)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "436af326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest accuracy in the validation accuracy  0.6194368755676658\n",
      "best hyperparameters: {'C': 0.23357214690901212, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# We fit the model and print out the highest accuracy in the validation data and the corresponding best parameter:\n",
    "clf.fit(X, y)\n",
    "print(\"highest accuracy in the validation accuracy \",clf.best_score_)\n",
    "print(\"best hyperparameters:\",clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f92c474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best parameters accuracy score : 0.6497737556561086\n"
     ]
    }
   ],
   "source": [
    "# We also find the accuracy using the test data we see it's much higher than the validation data:\n",
    "print(\" best parameters accuracy score :\",clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8b06e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "860444a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ...,  1, -1, -1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0131c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each sample corresponds the predicted class or predicted sentiment.\n",
    "\n",
    "# It turns out the logistic regression is calculating the score using parameters  ùë§ùëñùëó\n",
    "#   for the  ùëó\n",
    "#   class\n",
    "\n",
    "# ùë†ùëó=‚àëùëÅùëñ=1ùë§ùëñùëóùë•ùëñ\n",
    " \n",
    "# The larger  ùë†ùëó\n",
    "#  , the higher the score, and the more likely the sample belongs to that class. As  ùë•ùëñ\n",
    "#   is usually one, a significant  ùë§ùëñùëó\n",
    "#   means that word is more likely to contribute to that class; if  ùë§ùëñùëó\n",
    "#   is negative, the more likely that word is not to belong to that class.\n",
    "\n",
    "# We can determine the words with the largest parameters or score, for each class  ùëó\n",
    "#   we sort the index of each parameter  ùë§ùëñùëó\n",
    "#   from largest to smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9736b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = np.argsort(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "875b05b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(j) or y= 0\n",
      "['solid' 'powerful' 'enjoyable' 'best' 'hilarious' 'fun' 'perfectly'\n",
      " 'charming' 'always' 'human']\n",
      "(j) or y= 1\n",
      "['offers' 'screen' 'imagine' 'thoroughly' 'crafted' 'impressive' 'going'\n",
      " 'frequently' 'watching' 'four']\n",
      "(j) or y= 2\n",
      "['suffers' 'dull' 'worst' 'mess' 'too' 'unfortunately' 'lack' 'less'\n",
      " 'plain' 'bad']\n"
     ]
    }
   ],
   "source": [
    "# We find the corresponding vector  ùë•ùëñ\n",
    "#   and then using get_feature_names_out() to find the word with the highest score\n",
    "for class_ in range(3):\n",
    "    \n",
    "    print(\"(j) or y=\",class_)\n",
    "    print(np.array(vectorizer.get_feature_names())[feature_importance[class_,0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19b354fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 Create you own CountVectorizer function\n",
    "\n",
    "# Question 1 a) Create a function that takes the input column of a dataframe train_dataset[\"X\"] .\n",
    "# The output is a dictionary where each key is a word in the corpus, and the value is a unique digit, \n",
    "# then apply the function and call the output word_to_idx\n",
    "def build_feature_map(X):\n",
    "\n",
    "    word_types =set()\n",
    "    #Split string into words usig split() then apply(set) \n",
    "    for x in X.str.casefold().str.split().apply(set):\n",
    " \n",
    "    \n",
    "        word_types=word_types.union(x)\n",
    "    \n",
    "    # Create a dictionary keyed by word mapping it to an index\n",
    "    return   {word: idx for idx, word in enumerate(word_types)}\n",
    "\n",
    "word_to_idx = build_feature_map(train_dataset[\"X\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90a3826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the following to map the index to the key and the word to the value idx_to_word_func =\n",
    "# lambda word_to_idx :{v:k for k,v in word_to_idx.items()}\n",
    "\n",
    "# Question 1 b) write a function that outputs a TDM given train_dataset[\"X\"] you can use word_to_idx as an \n",
    "# input, apply the function to train_dataset[\"X\"] call the result X_train:\n",
    "def extract_features(word_to_idx, X):\n",
    "    D=len(word_to_idx)\n",
    "    N=X.shape[0]\n",
    "    words=set(word_to_idx.keys())\n",
    "    \n",
    "    features = dok_matrix((N, D))\n",
    "    for row,x in enumerate(X[0:]):\n",
    "        for word in x.split():\n",
    "            if word in words:\n",
    "                features[row,word_to_idx[word.casefold()]]+=1\n",
    "    return features\n",
    "\n",
    "X_train= extract_features(word_to_idx, train_dataset[\"X\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "780f8940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy  0.5912806539509536\n",
      "test accuracy  0.6339366515837104\n"
     ]
    }
   ],
   "source": [
    "# Question 2 Create a logistic regression object and train it using the training data. \n",
    "# Calculate the score using the validation data and test data\n",
    "X_train= extract_features(word_to_idx, train_dataset[\"X\"])\n",
    "y_train=train_dataset[[\"y\"]]\n",
    "\n",
    "X_val=extract_features(word_to_idx, validation_dataset[\"X\"])\n",
    "y_val=validation_dataset[['y']]\n",
    "X_test=extract_features(word_to_idx, test_dataset[\"X\"])\n",
    "y_test=test_dataset[['y']]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"validation accuracy \",lr.score(X_val,y_val))\n",
    "print(\"test accuracy \",lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e3d607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lawrence/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Bag-Of-Word Hyperparameters¬∂\n",
    "# Bag-Of-Word has several hyperparameters you can change to improve performance; most have to do with \n",
    "# reducing the number of dimensions in the TDM this improves performance, lets review some\n",
    "# Stop words\n",
    "\n",
    "# Stop words, which include \"and,\" \"the,\" and \"his,\" are seen to be uninformative in describing the content of a\n",
    "# document and may be eliminated sometimes to improve performance. However, removing stop words does not always \n",
    "# help with performance, so the validation data should be used to determine their effectiveness. A list of stop words\n",
    "# are available using Natural Language Toolkit (nltk) we can download the list of stop words as follows:\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc551c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# We specify the languish as English. The result is a list of stop words; we can print out the first ten:\n",
    "print(stopwords.words('english')[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a9a3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the toy data example, we add the list of stop words to the parameter stop_words and transform the data:\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "X_toy = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare the TDM with the original TDM with no stop words we see the columns corresponding to and, \n",
    "# the and this are missing:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
